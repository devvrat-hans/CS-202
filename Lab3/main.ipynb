{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "42c409ef",
   "metadata": {},
   "source": [
    "# Lab Assignment 3: Code Quality and Change Magnitude Analysis\n",
    "\n",
    "**Course:** CS202 Software Tools and Techniques for CSE\n",
    "\n",
    "## Objective\n",
    "This lab builds upon Lab 2's dataset to analyze code quality metrics, change magnitude, and semantic similarities in bug-fixing commits. We use various tools including Radon for complexity analysis and CodeBERT for semantic similarity.\n",
    "\n",
    "## Starting Point\n",
    "Using the file-level dataset from Lab 2 containing commit information, file changes, and LLM inferences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "851f92b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from collections import Counter, defaultdict\n",
    "import os\n",
    "\n",
    "# Repository analysis\n",
    "from pydriller import Repository\n",
    "\n",
    "# Code quality metrics\n",
    "from radon.complexity import cc_visit\n",
    "from radon.metrics import mi_visit\n",
    "from radon.raw import analyze\n",
    "\n",
    "# Semantic analysis\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "\n",
    "# Set repository path\n",
    "repo_url = 'PDFMathTranslate'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "740a6474",
   "metadata": {},
   "source": [
    "## Part A: Load Lab 2 Dataset\n",
    "\n",
    "Starting with the file-level dataset from the previous lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c2a9002",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: This assumes you have the Lab 2 dataset available\n",
    "# If not available, we'll create a sample dataset for demonstration\n",
    "\n",
    "try:\n",
    "    # Try to load existing Lab 2 data\n",
    "    lab2_data = pd.read_csv(\"../Lab2/master_commits_dataset.csv\")\n",
    "    print(f\"Loaded Lab 2 dataset with {len(lab2_data)} entries\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Lab 2 dataset not found. Creating sample data from repository...\")\n",
    "    # Create sample data if Lab 2 dataset is not available\n",
    "    lab2_data = pd.DataFrame({\n",
    "        'hash': ['sample_hash_1', 'sample_hash_2'],\n",
    "        'filename': ['file1.py', 'file2.py'],\n",
    "        'source_code_before': ['def old_func():\\n    pass', 'class OldClass:\\n    pass'],\n",
    "        'source_code_current': ['def new_func():\\n    return True', 'class NewClass:\\n    def method(self):\\n        return 1'],\n",
    "        'diff': ['sample diff 1', 'sample diff 2'],\n",
    "        'llm_inference': ['fix type 1', 'fix type 2'],\n",
    "        'rectified_message': ['Fixed function', 'Updated class']\n",
    "    })\n",
    "\n",
    "print(f\"Dataset columns: {list(lab2_data.columns)}\")\n",
    "print(f\"Dataset shape: {lab2_data.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c396e0b4",
   "metadata": {},
   "source": [
    "## Part B: Baseline Descriptive Statistics\n",
    "\n",
    "Computing and reporting baseline statistics about the repository and commits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ead6b523",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_repository_stats(repo_url):\n",
    "    \"\"\"Calculate comprehensive repository statistics.\"\"\"\n",
    "    \n",
    "    # Initialize counters\n",
    "    total_commits = 0\n",
    "    total_files = 0\n",
    "    files_per_commit = []\n",
    "    fix_types = Counter()\n",
    "    extension_counts = Counter()\n",
    "    unique_files = set()\n",
    "    \n",
    "    print(\"Analyzing repository...\")\n",
    "    \n",
    "    for commit in Repository(repo_url).traverse_commits():\n",
    "        total_commits += 1\n",
    "        commit_file_count = len(commit.modified_files)\n",
    "        files_per_commit.append(commit_file_count)\n",
    "        total_files += commit_file_count\n",
    "        \n",
    "        # Track unique files and extensions\n",
    "        for modified_file in commit.modified_files:\n",
    "            file_path = modified_file.new_path or modified_file.old_path\n",
    "            if file_path:\n",
    "                unique_files.add(file_path)\n",
    "                if '.' in file_path:\n",
    "                    extension = os.path.splitext(file_path)[1]\n",
    "                    extension_counts[extension] += 1\n",
    "    \n",
    "    # Calculate statistics\n",
    "    stats = {\n",
    "        'total_commits': total_commits,\n",
    "        'total_unique_files': len(unique_files),\n",
    "        'total_file_modifications': total_files,\n",
    "        'avg_files_per_commit': np.mean(files_per_commit) if files_per_commit else 0,\n",
    "        'median_files_per_commit': np.median(files_per_commit) if files_per_commit else 0,\n",
    "        'most_common_extensions': dict(extension_counts.most_common(10))\n",
    "    }\n",
    "    \n",
    "    return stats\n",
    "\n",
    "# Calculate and display repository statistics\n",
    "repo_stats = calculate_repository_stats(repo_url)\n",
    "\n",
    "print(\"\\n=== Repository Statistics ===\")\n",
    "print(f\"Total commits: {repo_stats['total_commits']:,}\")\n",
    "print(f\"Total unique files: {repo_stats['total_unique_files']:,}\")\n",
    "print(f\"Total file modifications: {repo_stats['total_file_modifications']:,}\")\n",
    "print(f\"Average files per commit: {repo_stats['avg_files_per_commit']:.2f}\")\n",
    "print(f\"Median files per commit: {repo_stats['median_files_per_commit']:.2f}\")\n",
    "print(f\"\\nMost common file extensions:\")\n",
    "for ext, count in repo_stats['most_common_extensions'].items():\n",
    "    print(f\"  {ext}: {count:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "260f8eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save repository statistics to CSV\n",
    "repo_stats_df = pd.DataFrame([repo_stats])\n",
    "repo_stats_df.to_csv(\"repository_statistics.csv\", index=False)\n",
    "print(\"\\nRepository statistics saved to 'repository_statistics.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5425cf4",
   "metadata": {},
   "source": [
    "## Part C: Structural Metrics with Radon\n",
    "\n",
    "Computing code quality metrics using Radon for before and after code versions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29f191c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_code_metrics(source_code):\n",
    "    \"\"\"Analyze code metrics using Radon.\"\"\"\n",
    "    if not source_code or source_code.strip() == \"\":\n",
    "        return {\"MI\": None, \"CC\": None, \"LOC\": None}\n",
    "    \n",
    "    try:\n",
    "        # Maintainability Index\n",
    "        mi = mi_visit(source_code, True)\n",
    "        \n",
    "        # Cyclomatic Complexity\n",
    "        cc_blocks = cc_visit(source_code)\n",
    "        cc_total = sum(block.complexity for block in cc_blocks)\n",
    "        \n",
    "        # Lines of Code\n",
    "        raw_metrics = analyze(source_code)\n",
    "        loc = raw_metrics.loc\n",
    "        \n",
    "        return {\"MI\": mi, \"CC\": cc_total, \"LOC\": loc}\n",
    "    except Exception as e:\n",
    "        print(f\"Error analyzing code metrics: {e}\")\n",
    "        return {\"MI\": None, \"CC\": None, \"LOC\": None}\n",
    "\n",
    "def calculate_metric_changes(before_metrics, after_metrics):\n",
    "    \"\"\"Calculate changes in metrics.\"\"\"\n",
    "    changes = {}\n",
    "    \n",
    "    for metric in ['MI', 'CC', 'LOC']:\n",
    "        before_val = before_metrics.get(metric)\n",
    "        after_val = after_metrics.get(metric)\n",
    "        \n",
    "        if before_val is not None and after_val is not None:\n",
    "            changes[f\"{metric}_Change\"] = after_val - before_val\n",
    "        else:\n",
    "            changes[f\"{metric}_Change\"] = None\n",
    "    \n",
    "    return changes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eed04379",
   "metadata": {},
   "source": [
    "## Part D: Change Magnitude Metrics\n",
    "\n",
    "Computing semantic and token similarities using CodeBERT and BLEU score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f636b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CodeBERT for semantic similarity\n",
    "print(\"Loading CodeBERT model...\")\n",
    "codebert_tokenizer = AutoTokenizer.from_pretrained(\"microsoft/codebert-base\")\n",
    "codebert_model = AutoModel.from_pretrained(\"microsoft/codebert-base\")\n",
    "\n",
    "def get_codebert_embedding(code):\n",
    "    \"\"\"Get CodeBERT embedding for code.\"\"\"\n",
    "    if not code or code.strip() == \"\":\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        inputs = codebert_tokenizer(\n",
    "            code, \n",
    "            return_tensors=\"pt\", \n",
    "            truncation=True, \n",
    "            max_length=512\n",
    "        )\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = codebert_model(**inputs)\n",
    "            # Use mean pooling over the sequence dimension\n",
    "            embedding = outputs.last_hidden_state.mean(dim=1).squeeze().numpy()\n",
    "        \n",
    "        return embedding\n",
    "    except Exception as e:\n",
    "        print(f\"Error getting CodeBERT embedding: {e}\")\n",
    "        return None\n",
    "\n",
    "def calculate_semantic_similarity(code1, code2):\n",
    "    \"\"\"Calculate semantic similarity using CodeBERT.\"\"\"\n",
    "    emb1 = get_codebert_embedding(code1)\n",
    "    emb2 = get_codebert_embedding(code2)\n",
    "    \n",
    "    if emb1 is None or emb2 is None:\n",
    "        return None\n",
    "    \n",
    "    # Cosine similarity\n",
    "    similarity = float(\n",
    "        torch.nn.functional.cosine_similarity(\n",
    "            torch.tensor(emb1), \n",
    "            torch.tensor(emb2), \n",
    "            dim=0\n",
    "        ).item()\n",
    "    )\n",
    "    \n",
    "    return similarity\n",
    "\n",
    "def calculate_token_similarity(code1, code2):\n",
    "    \"\"\"Calculate token similarity using BLEU score.\"\"\"\n",
    "    if not code1 or not code2:\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        reference = code1.split()\n",
    "        candidate = code2.split()\n",
    "        \n",
    "        if not reference or not candidate:\n",
    "            return None\n",
    "        \n",
    "        smoothing = SmoothingFunction().method1\n",
    "        bleu_score = sentence_bleu([reference], candidate, smoothing_function=smoothing)\n",
    "        \n",
    "        return bleu_score\n",
    "    except Exception as e:\n",
    "        print(f\"Error calculating BLEU score: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06d0e9aa",
   "metadata": {},
   "source": [
    "## Part E: Complete Analysis Pipeline\n",
    "\n",
    "Running the complete analysis on repository data and generating comprehensive metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0711815",
   "metadata": {},
   "outputs": [],
   "source": [
    "def comprehensive_code_analysis(repo_url, output_file=\"comprehensive_analysis.csv\", limit=100):\n",
    "    \"\"\"Run comprehensive analysis on repository commits.\"\"\"\n",
    "    \n",
    "    results = []\n",
    "    processed = 0\n",
    "    \n",
    "    print(f\"Starting comprehensive analysis (limit: {limit} commits)...\")\n",
    "    \n",
    "    for commit in Repository(repo_url).traverse_commits():\n",
    "        if processed >= limit:\n",
    "            break\n",
    "            \n",
    "        for modified_file in commit.modified_files:\n",
    "            file_path = modified_file.new_path or modified_file.old_path\n",
    "            \n",
    "            # Skip non-Python files for code metrics\n",
    "            if not file_path or not file_path.endswith('.py'):\n",
    "                continue\n",
    "            \n",
    "            before_code = modified_file.source_code_before\n",
    "            after_code = modified_file.source_code\n",
    "            \n",
    "            # Calculate code metrics\n",
    "            before_metrics = analyze_code_metrics(before_code)\n",
    "            after_metrics = analyze_code_metrics(after_code)\n",
    "            metric_changes = calculate_metric_changes(before_metrics, after_metrics)\n",
    "            \n",
    "            # Calculate similarity metrics\n",
    "            semantic_sim = calculate_semantic_similarity(before_code or \"\", after_code or \"\")\n",
    "            token_sim = calculate_token_similarity(before_code or \"\", after_code or \"\")\n",
    "            \n",
    "            # Prepare result entry\n",
    "            entry = {\n",
    "                \"commit_hash\": commit.hash,\n",
    "                \"file_path\": file_path,\n",
    "                \"commit_message\": commit.msg,\n",
    "                \n",
    "                # Before metrics\n",
    "                \"MI_Before\": before_metrics[\"MI\"],\n",
    "                \"CC_Before\": before_metrics[\"CC\"],\n",
    "                \"LOC_Before\": before_metrics[\"LOC\"],\n",
    "                \n",
    "                # After metrics\n",
    "                \"MI_After\": after_metrics[\"MI\"],\n",
    "                \"CC_After\": after_metrics[\"CC\"],\n",
    "                \"LOC_After\": after_metrics[\"LOC\"],\n",
    "                \n",
    "                # Changes\n",
    "                \"MI_Change\": metric_changes[\"MI_Change\"],\n",
    "                \"CC_Change\": metric_changes[\"CC_Change\"],\n",
    "                \"LOC_Change\": metric_changes[\"LOC_Change\"],\n",
    "                \n",
    "                # Similarity metrics\n",
    "                \"Semantic_Similarity\": semantic_sim,\n",
    "                \"Token_Similarity\": token_sim,\n",
    "            }\n",
    "            \n",
    "            results.append(entry)\n",
    "        \n",
    "        processed += 1\n",
    "        \n",
    "        if processed % 10 == 0:\n",
    "            print(f\"Processed {processed} commits...\")\n",
    "    \n",
    "    # Create DataFrame and save\n",
    "    df = pd.DataFrame(results)\n",
    "    df.to_csv(output_file, index=False)\n",
    "    \n",
    "    print(f\"\\nAnalysis complete! Saved {len(df)} entries to '{output_file}'\")\n",
    "    return df\n",
    "\n",
    "# Run comprehensive analysis\n",
    "analysis_df = comprehensive_code_analysis(repo_url, \"code_quality_analysis.csv\", limit=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f405cc0",
   "metadata": {},
   "source": [
    "## Part F: Classification & Agreement Analysis\n",
    "\n",
    "Classifying changes as Major/Minor based on similarity thresholds and analyzing agreement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76bd15d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_changes(df, semantic_threshold=0.7, token_threshold=0.7):\n",
    "    \"\"\"Classify changes as Major/Minor based on similarity thresholds.\"\"\"\n",
    "    \n",
    "    # Create classification columns\n",
    "    df['Semantic_Class'] = df['Semantic_Similarity'].apply(\n",
    "        lambda x: 'Minor' if x is not None and x >= semantic_threshold else 'Major'\n",
    "    )\n",
    "    \n",
    "    df['Token_Class'] = df['Token_Similarity'].apply(\n",
    "        lambda x: 'Minor' if x is not None and x >= token_threshold else 'Major'\n",
    "    )\n",
    "    \n",
    "    # Check agreement between classifications\n",
    "    df['Classes_Agree'] = df.apply(\n",
    "        lambda row: 'YES' if row['Semantic_Class'] == row['Token_Class'] else 'NO',\n",
    "        axis=1\n",
    "    )\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Apply classifications\n",
    "analysis_df = classify_changes(analysis_df)\n",
    "\n",
    "# Display classification statistics\n",
    "print(\"\\n=== Classification Results ===\")\n",
    "print(f\"Semantic Classification Distribution:\")\n",
    "print(analysis_df['Semantic_Class'].value_counts())\n",
    "\n",
    "print(f\"\\nToken Classification Distribution:\")\n",
    "print(analysis_df['Token_Class'].value_counts())\n",
    "\n",
    "print(f\"\\nClassification Agreement:\")\n",
    "print(analysis_df['Classes_Agree'].value_counts())\n",
    "\n",
    "agreement_rate = (analysis_df['Classes_Agree'] == 'YES').mean() * 100\n",
    "print(f\"\\nOverall Agreement Rate: {agreement_rate:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeef996a",
   "metadata": {},
   "source": [
    "## Visualization and Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14b03828",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive visualizations\n",
    "fig, axes = plt.subplots(3, 3, figsize=(15, 12))\n",
    "fig.suptitle('Code Quality and Similarity Analysis', fontsize=16)\n",
    "\n",
    "# Filter out None values for plotting\n",
    "valid_data = analysis_df.dropna()\n",
    "\n",
    "if not valid_data.empty:\n",
    "    # Maintainability Index plots\n",
    "    if 'MI_Before' in valid_data.columns:\n",
    "        axes[0, 0].hist(valid_data['MI_Before'].dropna(), bins=20, alpha=0.7, color='blue')\n",
    "        axes[0, 0].set_title('MI Before Distribution')\n",
    "        axes[0, 0].set_xlabel('Maintainability Index')\n",
    "    \n",
    "    if 'MI_After' in valid_data.columns:\n",
    "        axes[0, 1].hist(valid_data['MI_After'].dropna(), bins=20, alpha=0.7, color='green')\n",
    "        axes[0, 1].set_title('MI After Distribution')\n",
    "        axes[0, 1].set_xlabel('Maintainability Index')\n",
    "    \n",
    "    if 'MI_Change' in valid_data.columns:\n",
    "        axes[0, 2].hist(valid_data['MI_Change'].dropna(), bins=20, alpha=0.7, color='red')\n",
    "        axes[0, 2].set_title('MI Change Distribution')\n",
    "        axes[0, 2].set_xlabel('MI Change')\n",
    "    \n",
    "    # Cyclomatic Complexity plots\n",
    "    if 'CC_Change' in valid_data.columns:\n",
    "        axes[1, 0].hist(valid_data['CC_Change'].dropna(), bins=20, alpha=0.7, color='purple')\n",
    "        axes[1, 0].set_title('CC Change Distribution')\n",
    "        axes[1, 0].set_xlabel('Complexity Change')\n",
    "    \n",
    "    # Lines of Code plots\n",
    "    if 'LOC_Change' in valid_data.columns:\n",
    "        axes[1, 1].hist(valid_data['LOC_Change'].dropna(), bins=20, alpha=0.7, color='orange')\n",
    "        axes[1, 1].set_title('LOC Change Distribution')\n",
    "        axes[1, 1].set_xlabel('LOC Change')\n",
    "    \n",
    "    # Similarity plots\n",
    "    if 'Semantic_Similarity' in valid_data.columns:\n",
    "        axes[1, 2].hist(valid_data['Semantic_Similarity'].dropna(), bins=20, alpha=0.7, color='cyan')\n",
    "        axes[1, 2].set_title('Semantic Similarity Distribution')\n",
    "        axes[1, 2].set_xlabel('Semantic Similarity')\n",
    "    \n",
    "    if 'Token_Similarity' in valid_data.columns:\n",
    "        axes[2, 0].hist(valid_data['Token_Similarity'].dropna(), bins=20, alpha=0.7, color='magenta')\n",
    "        axes[2, 0].set_title('Token Similarity Distribution')\n",
    "        axes[2, 0].set_xlabel('Token Similarity')\n",
    "    \n",
    "    # Classification agreement\n",
    "    if 'Classes_Agree' in valid_data.columns:\n",
    "        agree_counts = valid_data['Classes_Agree'].value_counts()\n",
    "        axes[2, 1].pie(agree_counts.values, labels=agree_counts.index, autopct='%1.1f%%')\n",
    "        axes[2, 1].set_title('Classification Agreement')\n",
    "    \n",
    "    # Scatter plot: Semantic vs Token Similarity\n",
    "    if 'Semantic_Similarity' in valid_data.columns and 'Token_Similarity' in valid_data.columns:\n",
    "        semantic_clean = valid_data['Semantic_Similarity'].dropna()\n",
    "        token_clean = valid_data['Token_Similarity'].dropna()\n",
    "        \n",
    "        # Align the data\n",
    "        common_idx = semantic_clean.index.intersection(token_clean.index)\n",
    "        if len(common_idx) > 0:\n",
    "            axes[2, 2].scatter(semantic_clean[common_idx], token_clean[common_idx], alpha=0.6)\n",
    "            axes[2, 2].set_xlabel('Semantic Similarity')\n",
    "            axes[2, 2].set_ylabel('Token Similarity')\n",
    "            axes[2, 2].set_title('Semantic vs Token Similarity')\n",
    "            axes[2, 2].plot([0, 1], [0, 1], 'r--', alpha=0.5)  # Diagonal line\n",
    "\n",
    "else:\n",
    "    print(\"No valid data available for plotting\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03e5e7e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save final comprehensive dataset\n",
    "analysis_df.to_csv(\"final_code_analysis_results.csv\", index=False)\n",
    "\n",
    "# Display summary statistics\n",
    "print(\"\\n=== Final Analysis Summary ===\")\n",
    "print(f\"Total files analyzed: {len(analysis_df)}\")\n",
    "print(f\"Files with valid metrics: {analysis_df.dropna().shape[0]}\")\n",
    "\n",
    "# Correlation analysis\n",
    "numeric_cols = analysis_df.select_dtypes(include=[np.number]).columns\n",
    "correlation_matrix = analysis_df[numeric_cols].corr()\n",
    "\n",
    "print(f\"\\nCorrelations with Semantic Similarity:\")\n",
    "if 'Semantic_Similarity' in correlation_matrix.columns:\n",
    "    semantic_corr = correlation_matrix['Semantic_Similarity'].sort_values(ascending=False)\n",
    "    for col, corr in semantic_corr.items():\n",
    "        if col != 'Semantic_Similarity' and not pd.isna(corr):\n",
    "            print(f\"  {col}: {corr:.3f}\")\n",
    "\n",
    "print(f\"\\nDataset saved as 'final_code_analysis_results.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ee8051e",
   "metadata": {},
   "source": [
    "## Results Summary\n",
    "\n",
    "This notebook provides a comprehensive analysis of code changes including:\n",
    "\n",
    "### Key Metrics Computed:\n",
    "1. **Structural Metrics (Radon)**:\n",
    "   - Maintainability Index (MI)\n",
    "   - Cyclomatic Complexity (CC)\n",
    "   - Lines of Code (LOC)\n",
    "\n",
    "2. **Change Magnitude Metrics**:\n",
    "   - Semantic Similarity (CodeBERT)\n",
    "   - Token Similarity (BLEU)\n",
    "\n",
    "3. **Classification**:\n",
    "   - Major vs Minor changes based on similarity thresholds\n",
    "   - Agreement analysis between different classification methods\n",
    "\n",
    "### Key Datasets Generated:\n",
    "- `repository_statistics.csv`: Overall repository statistics\n",
    "- `code_quality_analysis.csv`: Detailed code quality metrics\n",
    "- `final_code_analysis_results.csv`: Complete analysis with classifications\n",
    "\n",
    "### Applications:\n",
    "- Code quality assessment\n",
    "- Change impact analysis\n",
    "- Software evolution studies\n",
    "- Automated code review systems\n",
    "- Technical debt assessment"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
